<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.55.6" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Multiple Flow Convergence &middot; The Panaderia</title>

  
  <link type="text/css" rel="stylesheet" href="https://saahilclaypool.github.io/panaderia/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://saahilclaypool.github.io/panaderia/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://saahilclaypool.github.io/panaderia/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://saahilclaypool.github.io/panaderia/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://saahilclaypool.github.io/panaderia/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="https://saahilclaypool.github.io/panaderia/favicon.ico">

  
  <link href="" rel="alternate" type="application/rss+xml" title="The Panaderia" />

  
</head>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://saahilclaypool.github.io/panaderia/"><h1>The Panaderia</h1></a>
      <p class="lead">
       A test bed for TCP congestion control research 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://saahilclaypool.github.io/panaderia/">Home</a> </li>
        <li><a href="https://saahilclaypool.github.io/panaderia/publications"> Publications </a></li><li><a href="https://github.com/SaahilClaypool/rpi"> Github Repository </a></li><li><a href="https://saahilclaypool.github.io/panaderia/background"> Background </a></li><li><a href="https://saahilclaypool.github.io/panaderia/config"> Configuration </a></li><li><a href="https://saahilclaypool.github.io/panaderia/validation"> Validation </a></li><li><a href="https://saahilclaypool.github.io/panaderia/results"> Results </a></li>
      </ul>
    </nav>

    <p>Copyright Â© 2019 Saahil Claypool</p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>Multiple Flow Convergence</h1>
  <time datetime=2018-11-10T16:26:54-0500 class="post-date">Sat, Nov 10, 2018</time>
  
  <aside>
    
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#raspberry-pi">Raspberry Pi</a></li>
<li><a href="#iperf3">Iperf3</a></li>
<li><a href="#simulating-a-80mbps-link">Simulating a 80mbps Link</a>
<ul>
<li><a href="#tc-traffic-control">TC: Traffic Control</a></li>
<li><a href="#token-bucket-filters-with-netem-delay">Token bucket filters with netem delay</a></li>
<li><a href="#netem-with-rate-control">Netem with Rate Control</a></li>
<li><a href="#tbf-with-peak-rate-limits">TBF with peak rate limits</a></li>
<li><a href="#cake">Cake</a></li>
</ul></li>
<li><a href="#final-solution-two-token-buckets">Final Solution: Two Token Buckets</a></li>
</ul></li>
</ul>
</nav>
  </aside>
  
  <p>Given multiple bbr flows competing at the same bottleneck, all of the flows should sync. BBR has a mechanism to
drain the queue and check for a lowest round trip time after some amount of seconds <em>since the last time the round trip time changed</em>. With multiple flows, they <em>all</em> should see the change in round trip time at the same time, so they should begin to drain the queue at the same time.</p>

<p>The goodput (throughput seen by the receiver) should looks something like this result published by the BBR team in <a href="https://cacm.acm.org/magazines/2017/2/212428-bbr-congestion-based-congestion-control/fulltext">this paper</a>:</p>

<p><img src="https://deliveryimages.acm.org/10.1145/3010000/3009824/figs/f8.jpg" alt="Syncing of Multiple Flows" /></p>

<hr/>

<p>And our results, which look roughly similar:</p>

<p><img src="https://saahilclaypool.github.io/panaderia/results/4_throughput.svg" alt="Our Results" /></p>

<p>See below for the configuration details and analysis</p>

<h2 id="raspberry-pi">Raspberry Pi</h2>

<p>Each client (churro 1 - 4) and server (tarta 1 - 4) is set to use BBR as their congest algorithm. This can be done with the commands:</p>

<pre><code class="language-sh">sudo sysctl net.core.default_qdisc=fq;
sudo sysctl net.ipv4.tcp_congestion_control={cc};
sudo sysctl net.ipv4.tcp_congestion_control;
</code></pre>

<h2 id="iperf3">Iperf3</h2>

<p>We create four iperf3 flows from the <code>churro</code> subnet to the <code>tarta</code> subnet. The bulk sender (the iperf3 client) connects from churro1 to tarta1 for each pair.</p>

<p>Server:</p>

<pre><code class="language-sh">iperf3 -s
</code></pre>

<p>Client on each churro:</p>

<pre><code class="language-sh">iperf3 -c tarta&lt;n&gt; -t 120
</code></pre>

<h2 id="simulating-a-80mbps-link">Simulating a 80mbps Link</h2>

<blockquote>
<p>Note: our raspberry pi testbed is running a slightly newer version of bbr than the one used to generate the graphs in this paper.</p>
</blockquote>

<h3 id="tc-traffic-control">TC: Traffic Control</h3>

<p>TC is a tool that allows us to control the traffic leaving the interface towards the <code>tarta</code> subnet. In <code>TC</code>,
there are a number of different <em>queueing disciplines</em> (qdisc) that can be used to control this traffic. We looked
at 4 different qdiscs to recreate the BBR results above.</p>

<h3 id="token-bucket-filters-with-netem-delay">Token bucket filters with netem delay</h3>

<p>The token bucket filter (tbf) for tc creates a bucket of tokens that generate at some rate. Packets that arrive
can only be sent using a token. This allows us to effectively control the rate of transmission for the interface to
the desired 80mbps.</p>

<p>To create the 10ms delay, we used <a href="http://man7.org/linux/man-pages/man8/tc-netem.8.html">Netem</a>,
a network emulator that can be used to add delay and simulate loss on an otherwise perfect
wired connection.</p>

<p>The setup for these two qdisc can be done as follows:</p>

<pre><code class="language-sh"># delete old rules
sudo tc qdisc del dev enp3s0 root

# Add the netem delay
sudo tc qdisc add dev enp3s0 root handle 1:0 netem delay 10ms
# Add the tbf as a child to the netem
sudo tc qdisc add dev enp3s0 parent 1:1 handle 10: tbf rate 80mbit buffer 1mbit limit 1000mbit 

tc -s qdisc ls dev enp3s0
</code></pre>

<p>And, here are the results:</p>

<p><img src="https://saahilclaypool.github.io/panaderia/80m_tbf.png" alt="Netem + TBF" /></p>

<p>Unfortunately, there are &lsquo;spikes&rsquo; in the graph after every phase where BBR is draining the queue. This could be because we are using a slightly different version of BBR than the original paper used, but we believe the real cause is the token bucket filter.</p>

<p>Because the token bucket filter fills up tokens at a constant rate and BBR underutilizes the link during the draining phase, the token bucket filter will generate a pool of tokens. So, when the first flow leaves the draining phase, it will be able to send all of its packets through this token bucket immediately as there is a surplus of tokens. This could be the cause of the spikes.</p>

<p>That being said, the convergence to a fair throughput is promising. But, we continued to explore other queue disciplines to remove these bursts.</p>

<h3 id="netem-with-rate-control">Netem with Rate Control</h3>

<p>After reading the netem documentation, it looks like netem natively supports rate control now. So, we tried to do a similar test with netem to see if this reduced the burst issues of the token bucket filter.</p>

<p>This was the setup:</p>

<pre><code class="language-sh">sudo tc qdisc del dev enp3s0 root
sudo tc qdisc add dev enp3s0 root handle 1:0 netem rate 80mbit delay 10ms
tc -s qdisc ls dev enp3s0
</code></pre>

<p>And this was the result:</p>

<p><img src="https://saahilclaypool.github.io/panaderia/80m_netem.png" alt="Netem Rate Control" /></p>

<p>While the flows did seem to sync, the throughput was all over the place. Additionally, despite the rate being capped at 80mbps and the first flow starting 2 seconds before the next flow, the maximum observed throughput is only around 30mbps. I <em>think</em> this is because netem is not provided consistent rate limiting or latency. the man page does provide this quote:</p>

<blockquote>
<p>The main known limitation of Netem are related to timer granularity, since Linux  is  not  a  real-time
operating system.</p>
</blockquote>

<p>which may indicate that the rate features are not completely stable (at least not at the precision we need). As this is a relatively new feature in netem, there is not much documentation on why we may be having these results.</p>

<h3 id="tbf-with-peak-rate-limits">TBF with peak rate limits</h3>

<p>As &lsquo;burstiness&rsquo; is a common problem with token bucket filters, there are ways built in to mitigate these issues. Specifically, you can set a <em>peak rate</em> for the token bucket. This should limit the <em>maximum rate</em> at which tokens can be used from the bucket. So, if this is set to or close to the bucket-filling rate, then this should eliminate the burst.</p>

<p>Our setup:</p>

<pre><code class="language-sh">sudo tc qdisc del dev enp3s0 root

sudo tc qdisc add dev enp3s0 root handle 1:0 netem delay 10ms
sudo tc qdisc add dev enp3s0 parent 1:1 handle 10: tbf rate 80mbit buffer 1mbit limit 1000mbit peakrate 81mbit mtu 1500

tc -s qdisc ls dev enp3s0
</code></pre>

<p>Note: we tried the mtu at a number of sizes larger than and smaller than the mtu of our target interface (1500).</p>

<p>Our Results:</p>

<p><img src="https://saahilclaypool.github.io/panaderia/80m_tbf_peak.png" alt="Peak Rate TBF" /></p>

<blockquote>
<p>Aborted early</p>
</blockquote>

<p>This trial resulted in zero throughput after the bucket was drained. We are still unsure why. We will continue exploring this.</p>

<h3 id="cake">Cake</h3>

<p><a href="https://www.bufferbloat.net/projects/codel/wiki/Cake/">Cake</a> is a more sophisticated queuing discipline for shaping traffic. While the results were initially promising (the graphs looked good), we believed that Cake was doing <em>too much</em> to shape the traffic, such as limiting the rate per flow (even with the settings off).</p>

<p>As this would make the results of our experiments harder to reason about, and thus effectively worthless for validating our test bed, we decided to not pursue cake further.</p>

<h2 id="final-solution-two-token-buckets">Final Solution: Two Token Buckets</h2>

<p>With a single token bucket, there needs to be some accumulation of tokens or the total rate which never reach the desired rate. To quote the man pages:
&gt; In general, larger  shaping rates require a larger buffer. For 10mbit/s on Intel, you need at least 10kbyte buffer if you want to reach your configured rate!</p>

<p>The idea behind the peak rate setting is to add a <em>second</em> token bucket with a much smaller buffer. This will catch the burst from the token bucket and ensure that the &lsquo;burst&rsquo; from the first token bucket is much smaller. Unfortunately, I was not able to get the <em>peak rate</em> setting on the tbf to work correcetly.</p>

<p>The solution we found was to manually create a second token bucket, shown below:</p>

<blockquote>
<p>Note: this simulation minimizes drops by setting the limit / queue size to be very large for each queuing discipline</p>
</blockquote>

<pre><code class="language-sh">sudo tc qdisc del dev enp3s0 root

sudo tc qdisc add dev enp3s0 root handle 1:0 netem delay 10ms limit 1000
sudo tc qdisc add dev enp3s0 parent 1:1 handle 10: tbf rate 80mbit buffer 1mbit limit 1000mbit 
# Add a second token bucket with a much smaller buffer / burst size
sudo tc qdisc add dev enp3s0 parent 10:1 handle 100: tbf rate 80mbit burst .05mbit limit 1000mbit  

tc -s qdisc ls dev enp3s0
</code></pre>

<p>This produces <em>much</em> smaller bursts in the graph, as shown here:</p>

<blockquote>
<p>Note: we use four flows at 80 mbps instead of 5 flows at 100 mbps. This is because we have 8 machines, and thus
can only test multiples of four flows with identical conditions.</p>
</blockquote>

<p><img src="https://saahilclaypool.github.io/panaderia/80m_multi_tbf.png" alt="small bursts" /></p>

<p>This behavior very closely matches the behavior produced by the google bbr team, albeit with very small bursts after each draining phase.</p>

<p>With more flows (16) the behavior continues to be quite reasonable.</p>

<p><img src="https://saahilclaypool.github.io/panaderia/results/16_throughput.svg" alt="many flows" /></p>
</div>


    </main>

    
  </body>
</html>
